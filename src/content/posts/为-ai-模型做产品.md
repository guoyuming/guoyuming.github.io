---
title: "为 AI 模型做产品"
---

**概率模型，一定会有幻想**

概率模型优势在自然语言处理和生成场景的泛化、通用能力，解决之前算法和逻辑无法覆盖的非标需求。

概率带来的幻觉，对于 100% 要求准确的 toB 场景，LLM 的幻想会致命，尤其是每个业务节点 90% 可用，90% * 90% * 90% 可能到最终可用程度只有 10%。而 toC 场景，比如情感陪伴、创意工具，LLM 的幻想不但可接受，反而会让用户发现意想不到的惊喜，带来增量价值。

降低幻觉的方法：Supervised Fine-tuning（监督微调），RAG, Fine-tuning, PE

**模型被语言数据训练，能力被语言激活**

模型层：通过 Supervised Fine-tuning（监督微调）, Reinforcement learning（强化学习） 来优化模型或使之更适合垂直领域。

应用层 PE：提示词很重要，系统提示词和用户提示词。系统提示词会被给予更高的优先级和权重，模型计算中有更多注意力。

应用层 PE：虽然 PE 很重要，但不能高估用户写提示词的能力和意愿。从一个空白文本框开始，反而很茫然，尽量不需要用户输入提示词，提示词一键扩写、自动优化、对比调试、提示词模版，通过系统信息、用户数据、上下文信息来给模型更精确指引。可以把提示词放在链路最后，作为给高级用户的选项，提升输出质量和差异化需求效果。

**模型推理和 API 调用延时**

应用层：通过工程方法，并行、异步等，来减少任务延时，否则无法商用；移动互联网时代，C 端用户习惯了不等待，AI 应用需要有新的交互方式，比如流式输出、等待期间对用户做 AI 教育

**模型成本高**

不同于传统应用，每次使用几乎免费。AI 应用面每次使用都有成本，产品设计时候，新增功能或场景，应该充分考虑成本计算和 cover 成本。用户端：帮用户选择最合适成本的模型、做 embedding、做通用 + 垂直、推理 + 对话生成模型混合使用的策略

**模型无状态、上下文窗口有限制**

模型本身无记忆，每次输入都是重头开始，不会记录历史输入或输出，每次调用都是独立运算。

产品上需要引入上下文管理、RAG、向量存储等方式，做记忆存储和调用。

记忆存储分长时记忆和短时记忆：

- 短时记忆：可以用 LLM 本身上下文窗口，但容量有限；更推荐的方式用另外一个 LLM 在来做对话总结摘要，做外挂记忆，在每次对话时候将外挂记忆加载到当前聊天或任务中。
- 长时记忆：向量数据库存储所有历史对话